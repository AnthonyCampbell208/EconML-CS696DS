{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numbers\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, check_cv, GridSearchCV, BaseCrossValidator, RandomizedSearchCV\n",
    "# TODO: conisder working around relying on sklearn implementation details\n",
    "from sklearn.model_selection._validation import (_check_is_permutation,\n",
    "                                                 _fit_and_predict)\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import indexable, check_random_state\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from model_selection_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data and create a model\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Initialize the GridSearchCV object\n",
    "param_grid = {}\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Initialize the RandomizedSearchCV object\n",
    "param_distributions = {'C': [0.1, 1, 10]}\n",
    "n_iter = 5\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_distributions, n_iter=n_iter, cv=5)\n",
    "\n",
    "# Test if they are instances of BaseSearchCV\n",
    "print(isinstance(grid_search, BaseSearchCV)) # True\n",
    "print(isinstance(random_search, BaseSearchCV)) # True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchEstimatorList(BaseEstimator):\n",
    "    def __init__(self, estimator_list = ['linear', 'forest'], param_grid_list = 'auto', is_discrete=False, scoring=None,\n",
    "                 n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',\n",
    "                 error_score=np.nan, return_train_score=False):\n",
    "        \n",
    "        self.estimator_list = get_complete_estimator_list(estimator_list, 'discrete' if is_discrete else 'continuous')\n",
    "\n",
    "        if param_grid_list == 'auto':\n",
    "            self.param_grid_list = auto_hyperparameters(estimator_list=self.estimator_list, is_discrete=is_discrete)\n",
    "        elif (param_grid_list == None) and (param_grid_list == 'default'):\n",
    "            self.param_grid_list = len(estimator_list) * [{}]\n",
    "        else:\n",
    "            self.param_grid_list = param_grid_list\n",
    "        # self.categorical_indices = categorical_indices\n",
    "        if scoring == None:\n",
    "            if is_discrete:\n",
    "                self.scoring = 'f1'\n",
    "            else:\n",
    "                self.scoring = 'mse'\n",
    "            warnings.warn(f\"No scoring value was given. Using default score method {self.scoring}.\")\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.refit = refit\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "        self.error_score = error_score\n",
    "        self.return_train_score = return_train_score\n",
    "        return\n",
    "\n",
    "    def select(self, X, y, *, scaling=True, sample_weight=None, groups=None):\n",
    "        \"\"\"\n",
    "        Perform cross-validation on the estimator list.\n",
    "        \"\"\"\n",
    "        self._search_list = []\n",
    "        self.scaling = scaling\n",
    "        if scaling:\n",
    "            if is_data_scaled(X):\n",
    "                warnings.warn(\"Data may already be scaled. Scaling twice may negatively affect results.\", UserWarning)\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X)\n",
    "            scaled_X = self.scaler.transform(X)\n",
    "\n",
    "        for estimator, param_grid in zip(self.estimator_list, self.param_grid_list):\n",
    "            try:\n",
    "                temp_search = GridSearchCV(estimator, param_grid, scoring=self.scoring,\n",
    "                                       n_jobs=self.n_jobs, refit=self.refit, cv=self.cv, verbose=self.verbose,\n",
    "                                       pre_dispatch=self.pre_dispatch, error_score=self.error_score,\n",
    "                                       return_train_score=self.return_train_score)\n",
    "                if scaling: # is_linear_model(estimator) and\n",
    "                    temp_search.fit(scaled_X, y, groups=groups, sample_weight=sample_weight) # , groups=groups, sample_weight=sample_weight\n",
    "                    self._search_list.append(temp_search)\n",
    "                else:\n",
    "                    temp_search.fit(X, y,  groups=groups, sample_weight=sample_weight)\n",
    "                    self._search_list.append(temp_search)\n",
    "            except (ValueError, TypeError, FitFailedWarning) as e:\n",
    "                # Raise a warning for the failed initialization\n",
    "                warning_msg = f\"Warning: {e} for estimator {estimator} and param_grid {param_grid}\"\n",
    "                warnings.warn(warning_msg, category=UserWarning)\n",
    "        self.best_ind_ = np.argmax([search.best_score_ for search in self._search_list])\n",
    "        self.best_estimator_ = self._search_list[self.best_ind_].best_estimator_\n",
    "        self.best_score_ = self._search_list[self.best_ind_].best_score_\n",
    "        self.best_params_ = self._search_list[self.best_ind_].best_params_\n",
    "        return self\n",
    "    \n",
    "    def scaler_transform(self, X):\n",
    "        if self.scaling:    \n",
    "            return self.scaler.transform(X)\n",
    "    def best_model(self):\n",
    "        return self.best_estimator_\n",
    "    def predict(self, X):\n",
    "        if self.scaling:    \n",
    "            return self.best_estimator_.predict(self.scaler.transform(X))\n",
    "        return self.best_estimator_.predict(X)\n",
    "    def predict_prob(self, X):\n",
    "        return self.best_estimator_.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'penalty': ['l1', 'l2', 'elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']}, {'n_estimators': [100, 500, 1000], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}]\n",
      "[{'penalty': ['l1', 'l2', 'elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']}, {'n_estimators': [100, 500, 1000], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}]\n",
      "{'penalty': ['l1', 'l2', 'elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']}\n",
      "{'n_estimators': [100, 500, 1000], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/32/km91km0x2s98wl2f_rb4gb0r0000gn/T/ipykernel_88734/379041481.py:21: UserWarning: No scoring value was given. Using default score method f1.\n",
      "  warnings.warn(f\"No scoring value was given. Using default score method {self.scoring}.\")\n",
      "/var/folders/32/km91km0x2s98wl2f_rb4gb0r0000gn/T/ipykernel_88734/379041481.py:62: UserWarning: Warning: Invalid parameter 'C' for estimator LogisticRegressionCV(). Valid parameters are: ['Cs', 'class_weight', 'cv', 'dual', 'fit_intercept', 'intercept_scaling', 'l1_ratios', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'refit', 'scoring', 'solver', 'tol', 'verbose']. for estimator LogisticRegressionCV() and param_grid {'penalty': ['l1', 'l2', 'elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']}\n",
      "  warnings.warn(warning_msg, category=UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LogisticRegressionCV(), RandomForestClassifier()]\n",
      "[{'penalty': ['l1', 'l2', 'elasticnet'], 'C': [0.01, 0.1, 1, 10, 100], 'solver': ['liblinear']}, {'n_estimators': [100, 500, 1000], 'max_depth': [None, 5, 10, 20], 'min_samples_split': [2, 5], 'min_samples_leaf': [1, 2]}]\n",
      "RandomForestClassifier(n_estimators=1000)\n",
      "{'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1000}\n",
      "Accuracy: 1.0\n",
      "[[0.004 0.    0.996]\n",
      " [0.019 0.167 0.814]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.018 0.152 0.83 ]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.019 0.185 0.796]\n",
      " [0.019 0.185 0.796]\n",
      " [0.019 0.185 0.796]\n",
      " [0.019 0.167 0.814]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.002 0.994]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.019 0.185 0.796]\n",
      " [0.004 0.    0.996]\n",
      " [0.018 0.152 0.83 ]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.004 0.    0.996]\n",
      " [0.019 0.167 0.814]\n",
      " [0.019 0.185 0.796]]\n"
     ]
    }
   ],
   "source": [
    "search = SearchEstimatorList(is_discrete=True)\n",
    "search.select(X_train, y_train)\n",
    "print(search.best_model())\n",
    "print(search.best_params_)\n",
    "y_pred = search.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(search.predict_prob(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "print(search.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearchCV == RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# def set_search_hyperparameters(search_object, hyperparameters):\n",
    "#     if isinstance(search_object, (RandomizedSearchCV, GridSearchCV)):\n",
    "#         print('hi')\n",
    "#         search_object.set_params(**hyperparameters)\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid search object\")\n",
    "\n",
    "# Example usage\n",
    "search = RandomizedSearchCV(None, None)\n",
    "\n",
    "if isinstance(search, (RandomizedSearchCV, GridSearchCV)):\n",
    "    print('hi')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=None, param_grid={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=None, param_grid={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=None, param_grid={})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV(None, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_data_scaled(X):\n",
    "    \"\"\"\n",
    "    Check if the input data is already centered and scaled using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        X array-like of shape (n_samples, n_features): The input data.\n",
    "\n",
    "    Returns:\n",
    "        is_scaled (bool): Whether the input data is already centered and scaled using StandardScaler or not.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the mean and standard deviation of the scaled data\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    print(mean)\n",
    "    print(std)\n",
    "    # Check if the mean is close to 0 and the standard deviation is close to 1\n",
    "    is_scaled = np.allclose(mean, 0.0) and np.allclose(std, 1.0)\n",
    "\n",
    "    return is_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.0, -1.0], [1.0, 0.0], [-1.0, 1.0]])\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "is_data_scaled(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scaled_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid should be a dict or a list, got: None of type NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:95\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[0;34m(self, param_grid)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, param_grid):\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_grid, (Mapping, Iterable)):\n\u001b[0;32m---> 95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid should be a dict or a list, got: \u001b[39m\u001b[39m{\u001b[39;00mparam_grid\u001b[39m!r}\u001b[39;00m\u001b[39m of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(param_grid)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(param_grid, Mapping):\n\u001b[1;32m    101\u001b[0m         \u001b[39m# wrap dictionary in a singleton list to support either dict\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# or list of dicts\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         param_grid \u001b[39m=\u001b[39m [param_grid]\n",
      "\u001b[0;31mTypeError\u001b[0m: Parameter grid should be a dict or a list, got: None of type NoneType"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonycampbell/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import (GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)\n",
    "\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.linear_model import (ARDRegression, BayesianRidge, ElasticNet,\n",
    "                                  Lars, Lasso, LassoLars, LinearRegression,\n",
    "                                  OrthogonalMatchingPursuit, Ridge)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, \\\n",
    "    OrthogonalMatchingPursuit, Lars, LassoLars, BayesianRidge, ARDRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "def is_linear_model(estimator):\n",
    "    \"\"\"\n",
    "    Check whether an estimator is a polynomial regression, logistic regression, linear SVM, or any other type of\n",
    "    linear model.\n",
    "\n",
    "    Parameters:\n",
    "    estimator (scikit-learn estimator): The estimator to check.\n",
    "\n",
    "    Returns:\n",
    "    is_linear (bool): True if the estimator is a linear model, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the estimator is a polynomial regression\n",
    "    if isinstance(estimator, Pipeline):\n",
    "        has_poly_feature_step = any(isinstance(step[1], PolynomialFeatures) for step in estimator.steps)\n",
    "        if has_poly_feature_step:\n",
    "            return True\n",
    "\n",
    "    # Check if the estimator is a linear regression or related model\n",
    "    if hasattr(estimator, 'fit_intercept') and hasattr(estimator, 'coef_'):\n",
    "        return True\n",
    "\n",
    "    # Check if the estimator is a logistic regression or linear SVM\n",
    "    if isinstance(estimator, (LogisticRegression, LinearSVC, SVC)):\n",
    "        return True\n",
    "\n",
    "    # Otherwise, the estimator is not a linear model\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(LinearRegression(), 'fit_intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_linear_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import (GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)\n",
    "\n",
    "def is_linear_model(estimator):\n",
    "    \"\"\"\n",
    "    Check whether an estimator is a polynomial regression or any other type of linear model.\n",
    "\n",
    "    Parameters:\n",
    "    estimator (scikit-learn estimator): The estimator to check.\n",
    "\n",
    "    Returns:\n",
    "    is_linear (bool): True if the estimator is a linear model, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the estimator is a polynomial regression\n",
    "    if isinstance(estimator, Pipeline):\n",
    "        has_poly_feature_step = any(isinstance(step[1], PolynomialFeatures) for step in estimator.steps)\n",
    "        if has_poly_feature_step:\n",
    "            return True\n",
    "\n",
    "    # Check if the estimator is any other type of linear model\n",
    "    if is_regressor(estimator) and isinstance(estimator, (LinearRegression, Ridge, Lasso, ElasticNet, \n",
    "                                                          OrthogonalMatchingPursuit, Lars, LassoLars, \n",
    "                                                          BayesianRidge, ARDRegression)):\n",
    "        return True\n",
    "\n",
    "    # Otherwise, the estimator is not a linear model\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m is_regressor\n\u001b[0;32m----> 3\u001b[0m is_linear_model(LinearRegression())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import is_regressor\n",
    "\n",
    "is_linear_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(BaseEstimator()) == elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "lr = [LinearRegression()]\n",
    "if not isinstance(lr[0], (str, BaseEstimator, BaseCrossValidator)):\n",
    "    raise TypeError(\"The list must contain only strings, sklearn model objects, and sklearn model selection objects.\")\n",
    "print(\"Yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1604001357.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def select(model_input, scale=False, normalize=False)\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def select_estimator(estimator_type, target_type):\n",
    "    if target_type not in ['continuous', 'discrete']:\n",
    "        raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "    if estimator_type == 'linear':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.linear_model.ElasticNetCV()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.linear_model.LogisticRegressionCV()\n",
    "    elif estimator_type == 'forest':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.RandomForestRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.RandomForestClassifier()\n",
    "    elif estimator_type == 'gbf':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.GradientBoostingRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.GradientBoostingClassifier()\n",
    "    elif estimator_type == 'nnet':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.neural_network.MLPRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.neural_network.MLPClassifier()\n",
    "    elif estimator_type == 'poly':\n",
    "        degrees = [2, 3, 4]  \n",
    "        models = []\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.linear_model.ElasticNetCV(precompute=True)\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.linear_model.LogisticRegressionCV()\n",
    "    elif estimator_type == 'automl':\n",
    "        return    \n",
    "    elif estimator_type == 'all':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.VotingRegressor(estimators=[\n",
    "                ('linear', select_estimator('linear', target_type)),\n",
    "                ('forest', select_estimator('forest', target_type)),\n",
    "                ('gbf', select_estimator('gbf', target_type)),\n",
    "                ('nnet', select_estimator('nnet', target_type)),\n",
    "                ('poly', select_estimator('poly', target_type)),\n",
    "            ], voting='soft')\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.VotingClassifier(estimators=[\n",
    "                ('linear', select_estimator('linear', target_type)),\n",
    "                ('forest', select_estimator('forest', target_type)),\n",
    "                ('gbf', select_estimator('gbf', target_type)),\n",
    "                ('nnet', select_estimator('nnet', target_type)),\n",
    "                ('poly', select_estimator('poly', target_type)),\n",
    "            ], voting='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# abc = ['a', 'b', 'c']\n",
    "abc = 'abc'\n",
    "if not isinstance(abc, list):\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# param_grid = {\n",
    "\n",
    "target_type = 'continuous'\n",
    "degrees = [2, 3, 4]  \n",
    "models = []\n",
    "for degree in degrees:\n",
    "    poly = sklearn.preprocessing.PolynomialFeatures(degree=degree)\n",
    "    if target_type == 'continuous':\n",
    "        linear = sklearn.linear_model.ElasticNetCV(precompute=True, cv=3, tol=0.1, verbose=1)\n",
    "    elif target_type == 'discrete':\n",
    "        linear = sklearn.linear_model.LogisticRegressionCV()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "    models.append((f\"poly{degree}\", sklearn.pipeline.Pipeline([('poly', poly), ('linear', linear)])))\n",
    "model = sklearn.ensemble.VotingRegressor(estimators=models)\n",
    "\n",
    "# generate some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566.204247883811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc09f31dac7c0d375489b816e1f71545b84c713f14d72a59d408a359f22c77a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
