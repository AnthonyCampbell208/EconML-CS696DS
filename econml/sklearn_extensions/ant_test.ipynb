{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numbers\n",
    "import warnings\n",
    "import sklearn\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.multiclass import type_of_target\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.base import clone, is_classifier\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, check_cv, GridSearchCV, BaseCrossValidator, RandomizedSearchCV\n",
    "# TODO: conisder working around relying on sklearn implementation details\n",
    "from sklearn.model_selection._validation import (_check_is_permutation,\n",
    "                                                 _fit_and_predict)\n",
    "from sklearn.exceptions import FitFailedWarning\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import indexable, check_random_state\n",
    "from sklearn.utils.validation import _num_samples\n",
    "from model_selection_utils import *\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection._search import BaseSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load data and create a model\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the sample pricing data\n",
    "file_url = \"https://msalicedatapublic.blob.core.windows.net/datasets/Pricing/pricing_sample.csv\"\n",
    "train_data = pd.read_csv(file_url)\n",
    "\n",
    "class SearchEstimatorList(BaseEstimator):\n",
    "    def __init__(self, estimator_list = ['linear', 'forest'], param_grid_list = 'auto', is_discrete=False, scoring=None,\n",
    "                 n_jobs=None, refit=True, cv=None, verbose=0, pre_dispatch='2*n_jobs',\n",
    "                 error_score=np.nan, return_train_score=False):\n",
    "\n",
    "        self.estimator_list = get_complete_estimator_list(estimator_list, 'discrete' if is_discrete else 'continuous')\n",
    "\n",
    "        if param_grid_list == 'auto':\n",
    "            self.param_grid_list = auto_hyperparameters(estimator_list=self.estimator_list, is_discrete=is_discrete)\n",
    "        elif (param_grid_list == None) and (param_grid_list == 'default'):\n",
    "            self.param_grid_list = len(estimator_list) * [{}]\n",
    "        else:\n",
    "            self.param_grid_list = param_grid_list\n",
    "        # self.categorical_indices = categorical_indices\n",
    "        if scoring == None:\n",
    "            if is_discrete:\n",
    "                self.scoring = 'f1'\n",
    "            else:\n",
    "                self.scoring = 'mse'\n",
    "            warnings.warn(f\"No scoring value was given. Using default score method {self.scoring}.\")\n",
    "        self.scoring = scoring\n",
    "        self.n_jobs = n_jobs\n",
    "        self.refit = refit\n",
    "        self.cv = cv\n",
    "        self.verbose = verbose\n",
    "        self.pre_dispatch = pre_dispatch\n",
    "        self.error_score = error_score\n",
    "        self.return_train_score = return_train_score\n",
    "        return\n",
    "\n",
    "    def select(self, X, y, *, scaling=True, sample_weight=None, groups=None):\n",
    "        \"\"\"\n",
    "        Perform cross-validation on the estimator list.\n",
    "        \"\"\"\n",
    "        self._search_list = []\n",
    "        self.scaling = scaling\n",
    "        if scaling:\n",
    "            if is_data_scaled(X):\n",
    "                warnings.warn(\"Data may already be scaled. Scaling twice may negatively affect results.\", UserWarning)\n",
    "            self.scaler = StandardScaler()\n",
    "            self.scaler.fit(X)\n",
    "            scaled_X = self.scaler.transform(X)\n",
    "\n",
    "        for estimator, param_grid in zip(self.estimator_list, self.param_grid_list):\n",
    "            try:\n",
    "                temp_search = GridSearchCV(estimator, param_grid, scoring=self.scoring,\n",
    "                                       n_jobs=self.n_jobs, refit=self.refit, cv=self.cv, verbose=self.verbose,\n",
    "                                       pre_dispatch=self.pre_dispatch, error_score=self.error_score,\n",
    "                                       return_train_score=self.return_train_score)\n",
    "                if scaling: # is_linear_model(estimator) and\n",
    "                    temp_search.fit(scaled_X, y, groups=groups) # , groups=groups, sample_weight=sample_weight\n",
    "                    self._search_list.append(temp_search)\n",
    "                else:\n",
    "                    temp_search.fit(X, y,  groups=groups)\n",
    "                    self._search_list.append(temp_search)\n",
    "            except (ValueError, TypeError, FitFailedWarning) as e:\n",
    "                # Raise a warning for the failed initialization\n",
    "                warning_msg = f\"Warning: {e} for estimator {estimator} and param_grid {param_grid}\"\n",
    "                warnings.warn(warning_msg, category=UserWarning)\n",
    "        self.best_ind_ = np.argmax([search.best_score_ for search in self._search_list])\n",
    "        self.best_estimator_ = self._search_list[self.best_ind_].best_estimator_\n",
    "        self.best_score_ = self._search_list[self.best_ind_].best_score_\n",
    "        self.best_params_ = self._search_list[self.best_ind_].best_params_\n",
    "        return self\n",
    "    \n",
    "    def scaler_transform(self, X):\n",
    "        if self.scaling:    \n",
    "            return self.scaler.transform(X)\n",
    "        \n",
    "    def best_model(self):\n",
    "        return self.best_estimator_\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.scaling:    \n",
    "            if is_data_scaled(X):\n",
    "                warnings.warn(\"Data may already be scaled. Scaling twice may negatively affect results.\", UserWarning)\n",
    "            return self.best_estimator_.predict(self.scaler.transform(X))\n",
    "        return self.best_estimator_.predict(X)\n",
    "    \n",
    "    def predict_prob(self, X):\n",
    "        if self.scaling:    \n",
    "            if is_data_scaled(X):\n",
    "                warnings.warn(\"Data may already be scaled. Scaling twice may negatively affect results.\", UserWarning)\n",
    "            return self.best_estimator_.predict(self.scaler.transform(X))\n",
    "        return self.best_estimator_.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n",
      "{}\n",
      "Accuracy: 1.0\n",
      "[[1.08199179e-11 6.08710397e-05 9.99939129e-01]\n",
      " [2.05936202e-04 7.59748394e-01 2.40045669e-01]\n",
      " [1.21083902e-18 7.79056317e-09 9.99999992e-01]\n",
      " [9.89532080e-12 4.12980878e-05 9.99958702e-01]\n",
      " [1.13793625e-12 2.63075021e-05 9.99973692e-01]\n",
      " [2.20941052e-04 7.48327461e-01 2.51451598e-01]\n",
      " [1.44061453e-09 7.13549217e-04 9.99286449e-01]\n",
      " [1.35745463e-14 8.88856199e-07 9.99999111e-01]\n",
      " [1.80763187e-12 2.78321604e-05 9.99972168e-01]\n",
      " [3.29220486e-10 4.15785487e-04 9.99584214e-01]\n",
      " [1.02639314e-13 2.35956353e-06 9.99997640e-01]\n",
      " [5.71709505e-04 8.82005356e-01 1.17422934e-01]\n",
      " [4.31926573e-04 9.06719661e-01 9.28484123e-02]\n",
      " [4.76955746e-04 8.61365240e-01 1.38157804e-01]\n",
      " [7.60422634e-04 8.34089362e-01 1.65150215e-01]\n",
      " [4.25246096e-12 2.34791069e-05 9.99976521e-01]\n",
      " [1.88349101e-15 1.96017846e-07 9.99999804e-01]\n",
      " [4.46495407e-10 5.01180008e-04 9.99498820e-01]\n",
      " [2.86635545e-11 7.17678328e-05 9.99928232e-01]\n",
      " [3.23142104e-15 2.86015892e-07 9.99999714e-01]\n",
      " [5.35328565e-04 7.87913320e-01 2.11551351e-01]\n",
      " [6.23426058e-13 6.33977349e-06 9.99993660e-01]\n",
      " [3.26302450e-04 6.94041140e-01 3.05632558e-01]\n",
      " [4.87080280e-15 3.91189771e-07 9.99999609e-01]\n",
      " [2.48909178e-16 1.40403867e-07 9.99999860e-01]\n",
      " [1.00409641e-14 6.46855891e-07 9.99999353e-01]\n",
      " [2.65292432e-15 5.19001056e-07 9.99999481e-01]\n",
      " [8.05651112e-16 1.27377192e-07 9.99999873e-01]\n",
      " [4.27092071e-04 8.00270239e-01 1.99302668e-01]\n",
      " [3.93784171e-04 7.79114260e-01 2.20491955e-01]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonycampbell/Documents/EconML-CS696DS/econml/sklearn_extensions/model_selection_utils.py:282: UserWarning: No hyperparameters for this type of model. Valid values are 'linear', 'forest', 'nnet', and 'poly'.\n",
      "  else:\n",
      "/var/folders/32/km91km0x2s98wl2f_rb4gb0r0000gn/T/ipykernel_52030/3576834498.py:20: UserWarning: No scoring value was given. Using default score method f1.\n",
      "  warnings.warn(f\"No scoring value was given. Using default score method {self.scoring}.\")\n"
     ]
    }
   ],
   "source": [
    "search = SearchEstimatorList(estimator_list = LogisticRegression(), is_discrete=True)\n",
    "search.select(X_train, y_train)\n",
    "print(search.best_model())\n",
    "print(search.best_params_)\n",
    "y_pred = search.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(search.predict_prob(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'linear__Cs': 10,\n",
       " 'linear__penalty': 'l2',\n",
       " 'linear__solver': 'saga',\n",
       " 'poly__degree': 2}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/32/km91km0x2s98wl2f_rb4gb0r0000gn/T/ipykernel_48526/2236011250.py:20: UserWarning: No scoring value was given. Using default score method f1.\n",
      "  warnings.warn(f\"No scoring value was given. Using default score method {self.scoring}.\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "attempt to get argmax of an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m search \u001b[39m=\u001b[39m SearchEstimatorList(estimator_list \u001b[39m=\u001b[39m [], is_discrete\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m search\u001b[39m.\u001b[39;49mselect(X_train, y_train)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(search\u001b[39m.\u001b[39mbest_model())\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(search\u001b[39m.\u001b[39mbest_params_)\n",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m, in \u001b[0;36mSearchEstimatorList.select\u001b[0;34m(self, X, y, scaling, sample_weight, groups)\u001b[0m\n\u001b[1;32m     58\u001b[0m         warning_msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWarning: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m for estimator \u001b[39m\u001b[39m{\u001b[39;00mestimator\u001b[39m}\u001b[39;00m\u001b[39m and param_grid \u001b[39m\u001b[39m{\u001b[39;00mparam_grid\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(warning_msg, category\u001b[39m=\u001b[39m\u001b[39mUserWarning\u001b[39;00m)\n\u001b[0;32m---> 60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_ind_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49margmax([search\u001b[39m.\u001b[39;49mbest_score_ \u001b[39mfor\u001b[39;49;00m search \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_search_list])\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search_list[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_ind_]\u001b[39m.\u001b[39mbest_estimator_\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_score_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_search_list[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_ind_]\u001b[39m.\u001b[39mbest_score_\n",
      "File \u001b[0;32m<__array_function__ internals>:5\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:1195\u001b[0m, in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[39m@array_function_dispatch\u001b[39m(_argmax_dispatcher)\n\u001b[1;32m   1122\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39margmax\u001b[39m(a, axis\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1123\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[39m    Returns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \n\u001b[1;32m   1194\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapfunc(a, \u001b[39m'\u001b[39;49m\u001b[39margmax\u001b[39;49m\u001b[39m'\u001b[39;49m, axis\u001b[39m=\u001b[39;49maxis, out\u001b[39m=\u001b[39;49mout)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:54\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     52\u001b[0m bound \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, method, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[39mif\u001b[39;00m bound \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m _wrapit(obj, method, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mreturn\u001b[39;00m bound(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:43\u001b[0m, in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[1;32m     42\u001b[0m     wrap \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39;49m(asarray(obj), method)(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     44\u001b[0m \u001b[39mif\u001b[39;00m wrap:\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(result, mu\u001b[39m.\u001b[39mndarray):\n",
      "\u001b[0;31mValueError\u001b[0m: attempt to get argmax of an empty sequence"
     ]
    }
   ],
   "source": [
    "search = SearchEstimatorList(estimator_list = [], is_discrete=True)\n",
    "search.select(X_train, y_train)\n",
    "print(search.best_model())\n",
    "print(search.best_params_)\n",
    "y_pred = search.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", acc)\n",
    "print(search.predict_prob(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(n_estimators=1000)\n"
     ]
    }
   ],
   "source": [
    "print(search.best_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomizedSearchCV == RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "# def set_search_hyperparameters(search_object, hyperparameters):\n",
    "#     if isinstance(search_object, (RandomizedSearchCV, GridSearchCV)):\n",
    "#         print('hi')\n",
    "#         search_object.set_params(**hyperparameters)\n",
    "#     else:\n",
    "#         raise ValueError(\"Invalid search object\")\n",
    "\n",
    "# Example usage\n",
    "search = RandomizedSearchCV(None, None)\n",
    "\n",
    "if isinstance(search, (RandomizedSearchCV, GridSearchCV)):\n",
    "    print('hi')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=None, param_grid={})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=None, param_grid={})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=None, param_grid={})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GridSearchCV(None, {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_data_scaled(X):\n",
    "    \"\"\"\n",
    "    Check if the input data is already centered and scaled using StandardScaler.\n",
    "\n",
    "    Args:\n",
    "        X array-like of shape (n_samples, n_features): The input data.\n",
    "\n",
    "    Returns:\n",
    "        is_scaled (bool): Whether the input data is already centered and scaled using StandardScaler or not.\n",
    "\n",
    "    \"\"\"\n",
    "    # Compute the mean and standard deviation of the scaled data\n",
    "    mean = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    print(mean)\n",
    "    print(std)\n",
    "    # Check if the mean is close to 0 and the standard deviation is close to 1\n",
    "    is_scaled = np.allclose(mean, 0.0) and np.allclose(std, 1.0)\n",
    "\n",
    "    return is_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[1. 1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[0.0, -1.0], [1.0, 0.0], [-1.0, 1.0]])\n",
    "scale = StandardScaler()\n",
    "scaled_X = scale.fit_transform(X)\n",
    "is_data_scaled(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(scaled_X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Parameter grid should be a dict or a list, got: None of type NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[1;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[0;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[1;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[1;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[1;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.9/site-packages/sklearn/model_selection/_search.py:95\u001b[0m, in \u001b[0;36mParameterGrid.__init__\u001b[0;34m(self, param_grid)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, param_grid):\n\u001b[1;32m     94\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(param_grid, (Mapping, Iterable)):\n\u001b[0;32m---> 95\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mParameter grid should be a dict or a list, got: \u001b[39m\u001b[39m{\u001b[39;00mparam_grid\u001b[39m!r}\u001b[39;00m\u001b[39m of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(param_grid)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         )\n\u001b[1;32m    100\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(param_grid, Mapping):\n\u001b[1;32m    101\u001b[0m         \u001b[39m# wrap dictionary in a singleton list to support either dict\u001b[39;00m\n\u001b[1;32m    102\u001b[0m         \u001b[39m# or list of dicts\u001b[39;00m\n\u001b[1;32m    103\u001b[0m         param_grid \u001b[39m=\u001b[39m [param_grid]\n",
      "\u001b[0;31mTypeError\u001b[0m: Parameter grid should be a dict or a list, got: None of type NoneType"
     ]
    }
   ],
   "source": [
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonycampbell/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import (GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)\n",
    "\n",
    "from sklearn.base import is_regressor\n",
    "from sklearn.linear_model import (ARDRegression, BayesianRidge, ElasticNet,\n",
    "                                  Lars, Lasso, LassoLars, LinearRegression,\n",
    "                                  OrthogonalMatchingPursuit, Ridge)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, \\\n",
    "    OrthogonalMatchingPursuit, Lars, LassoLars, BayesianRidge, ARDRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "def is_linear_model(estimator):\n",
    "    \"\"\"\n",
    "    Check whether an estimator is a polynomial regression, logistic regression, linear SVM, or any other type of\n",
    "    linear model.\n",
    "\n",
    "    Parameters:\n",
    "    estimator (scikit-learn estimator): The estimator to check.\n",
    "\n",
    "    Returns:\n",
    "    is_linear (bool): True if the estimator is a linear model, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the estimator is a polynomial regression\n",
    "    if isinstance(estimator, Pipeline):\n",
    "        has_poly_feature_step = any(isinstance(step[1], PolynomialFeatures) for step in estimator.steps)\n",
    "        if has_poly_feature_step:\n",
    "            return True\n",
    "\n",
    "    # Check if the estimator is a linear regression or related model\n",
    "    if hasattr(estimator, 'fit_intercept') and hasattr(estimator, 'coef_'):\n",
    "        return True\n",
    "\n",
    "    # Check if the estimator is a logistic regression or linear SVM\n",
    "    if isinstance(estimator, (LogisticRegression, LinearSVC, SVC)):\n",
    "        return True\n",
    "\n",
    "    # Otherwise, the estimator is not a linear model\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hasattr(LinearRegression(), 'fit_intercept')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_linear_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.ensemble\n",
    "import sklearn.linear_model\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.ensemble import (GradientBoostingClassifier,\n",
    "                              GradientBoostingRegressor,\n",
    "                              RandomForestClassifier, RandomForestRegressor)\n",
    "from sklearn.linear_model import ElasticNetCV, LogisticRegressionCV\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import (MaxAbsScaler, MinMaxScaler,\n",
    "                                   PolynomialFeatures, RobustScaler,\n",
    "                                   StandardScaler)\n",
    "\n",
    "def is_linear_model(estimator):\n",
    "    \"\"\"\n",
    "    Check whether an estimator is a polynomial regression or any other type of linear model.\n",
    "\n",
    "    Parameters:\n",
    "    estimator (scikit-learn estimator): The estimator to check.\n",
    "\n",
    "    Returns:\n",
    "    is_linear (bool): True if the estimator is a linear model, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the estimator is a polynomial regression\n",
    "    if isinstance(estimator, Pipeline):\n",
    "        has_poly_feature_step = any(isinstance(step[1], PolynomialFeatures) for step in estimator.steps)\n",
    "        if has_poly_feature_step:\n",
    "            return True\n",
    "\n",
    "    # Check if the estimator is any other type of linear model\n",
    "    if is_regressor(estimator) and isinstance(estimator, (LinearRegression, Ridge, Lasso, ElasticNet, \n",
    "                                                          OrthogonalMatchingPursuit, Lars, LassoLars, \n",
    "                                                          BayesianRidge, ARDRegression)):\n",
    "        return True\n",
    "\n",
    "    # Otherwise, the estimator is not a linear model\n",
    "    return False\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LinearRegression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m is_regressor\n\u001b[0;32m----> 3\u001b[0m is_linear_model(LinearRegression())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import is_regressor\n",
    "\n",
    "is_linear_model(LinearRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(BaseEstimator()) == elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yay\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "lr = [LinearRegression()]\n",
    "if not isinstance(lr[0], (str, BaseEstimator, BaseCrossValidator)):\n",
    "    raise TypeError(\"The list must contain only strings, sklearn model objects, and sklearn model selection objects.\")\n",
    "print(\"Yay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1604001357.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def select(model_input, scale=False, normalize=False)\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "\n",
    "def select_estimator(estimator_type, target_type):\n",
    "    if target_type not in ['continuous', 'discrete']:\n",
    "        raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "    if estimator_type == 'linear':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.linear_model.ElasticNetCV()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.linear_model.LogisticRegressionCV()\n",
    "    elif estimator_type == 'forest':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.RandomForestRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.RandomForestClassifier()\n",
    "    elif estimator_type == 'gbf':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.GradientBoostingRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.GradientBoostingClassifier()\n",
    "    elif estimator_type == 'nnet':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.neural_network.MLPRegressor()\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.neural_network.MLPClassifier()\n",
    "    elif estimator_type == 'poly':\n",
    "        degrees = [2, 3, 4]  \n",
    "        models = []\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.linear_model.ElasticNetCV(precompute=True)\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.linear_model.LogisticRegressionCV()\n",
    "    elif estimator_type == 'automl':\n",
    "        return    \n",
    "    elif estimator_type == 'all':\n",
    "        if target_type == 'continuous':\n",
    "            return sklearn.ensemble.VotingRegressor(estimators=[\n",
    "                ('linear', select_estimator('linear', target_type)),\n",
    "                ('forest', select_estimator('forest', target_type)),\n",
    "                ('gbf', select_estimator('gbf', target_type)),\n",
    "                ('nnet', select_estimator('nnet', target_type)),\n",
    "                ('poly', select_estimator('poly', target_type)),\n",
    "            ], voting='soft')\n",
    "        elif target_type == 'discrete':\n",
    "            return sklearn.ensemble.VotingClassifier(estimators=[\n",
    "                ('linear', select_estimator('linear', target_type)),\n",
    "                ('forest', select_estimator('forest', target_type)),\n",
    "                ('gbf', select_estimator('gbf', target_type)),\n",
    "                ('nnet', select_estimator('nnet', target_type)),\n",
    "                ('poly', select_estimator('poly', target_type)),\n",
    "            ], voting='soft')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": [
    "# abc = ['a', 'b', 'c']\n",
    "abc = 'abc'\n",
    "if not isinstance(abc, list):\n",
    "    print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "............................................................................................................................................................................................................................................................................................................[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.1min finished\n"
     ]
    }
   ],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.ensemble\n",
    "import sklearn.neural_network\n",
    "import sklearn.preprocessing\n",
    "import numpy as np\n",
    "import sklearn.pipeline\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# param_grid = {\n",
    "\n",
    "target_type = 'continuous'\n",
    "degrees = [2, 3, 4]  \n",
    "models = []\n",
    "for degree in degrees:\n",
    "    poly = sklearn.preprocessing.PolynomialFeatures(degree=degree)\n",
    "    if target_type == 'continuous':\n",
    "        linear = sklearn.linear_model.ElasticNetCV(precompute=True, cv=3, tol=0.1, verbose=1)\n",
    "    elif target_type == 'discrete':\n",
    "        linear = sklearn.linear_model.LogisticRegressionCV()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported target type: {target_type}\")\n",
    "    models.append((f\"poly{degree}\", sklearn.pipeline.Pipeline([('poly', poly), ('linear', linear)])))\n",
    "model = sklearn.ensemble.VotingRegressor(estimators=models)\n",
    "\n",
    "# generate some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# split the data into training and validation sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, train_size=0.8)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# predict on the test set\n",
    "y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1566.204247883811\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc09f31dac7c0d375489b816e1f71545b84c713f14d72a59d408a359f22c77a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
